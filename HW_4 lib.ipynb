{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to save file\n",
    "path = r'D:\\\\Data Science\\\\ADM(Algorithmic Methods of Data Mining)\\\\hw\\\\hw4\\\\'\n",
    "\n",
    "# the function will create the data and return it, just give the path to save the file\n",
    "\n",
    "def create_data(path):\n",
    "    # Create a dataframe to store all data\n",
    "\n",
    "    # find max page number\n",
    "    max_page_no = int(BeautifulSoup(requests.get(\"https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag=1\").text,\n",
    "                                    \"lxml\").find_all('span',attrs={\"class\": \"pagination__label\"})[-1:][0].text)\n",
    "\n",
    "    # save the file after each iteration with pickle \n",
    "    fileObject = open(path + \"adm-hw4-data-dene\",'wb')\n",
    "\n",
    "    for page in range(1,max_page_no+1): # go through for all pages\n",
    "\n",
    "        try:\n",
    "            url = \"https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag=\" + str(page)\n",
    "\n",
    "            print(\"page:\",page)\n",
    "\n",
    "            content = requests.get(url) # create a content for specified url\n",
    "            soup  = BeautifulSoup(content.text, \"lxml\") # turn it as lxml format\n",
    "\n",
    "            all_adv = soup.find_all('p',attrs={\"class\": \"titolo text-primary\"})\n",
    "\n",
    "            # store the data for only 1 page in a dataframe\n",
    "            data = pd.DataFrame(columns=[\"url\",\"price\", \"locali\", \"superficie\", \"bagni\", \"piano\", \"description\"])\n",
    "\n",
    "            for link_i in range(len(all_adv)):\n",
    "                link = all_adv[link_i].find('a').get('href') # get all link that are advertised on the page that we specified above\n",
    "\n",
    "                if 'https://' in link:\n",
    "                    linkURL = link\n",
    "                else: # some of link do not have http, http://, www.immobiliare.it or both parts. Thus, add it at the beginning of link\n",
    "                    if 'www.immobiliare.it' in link:\n",
    "                        if link[:2] == '//':\n",
    "                            linkURL = 'https:' + link\n",
    "                        elif link[0] == '/':\n",
    "                            linkURL = 'https:/' + link\n",
    "                    else:\n",
    "                        if link[0] != '/':\n",
    "                            linkURL = 'https://www.immobiliare.it/' + link\n",
    "                        else:\n",
    "                            linkURL = 'https://www.immobiliare.it' + link\n",
    "\n",
    "                print(linkURL)\n",
    "\n",
    "                link_content = requests.get(linkURL)\n",
    "                link_soup = BeautifulSoup(link_content.text, \"lxml\") # convert the content into lxml\n",
    "                ul = link_soup.find_all(\"ul\", attrs={\"class\": \"list-inline list-piped features__list\"}) # this list includes all features except price\n",
    "\n",
    "                # check which features having and store it if there is present or not\n",
    "                features = [] # at the end, it'll show that locali, superficie, bagni and piano exist in the link or not\n",
    "                all_features = list(map(lambda x: x.text, ul[0].find_all('div',attrs={\"class\": \"features__label\"}))) # which featers link have\n",
    "                features_check_list =['locali', 'superficie', 'bagni', 'piano']\n",
    "\n",
    "                for i in range(len(features_check_list)):\n",
    "                    if features_check_list[i] in all_features:\n",
    "                        features.append(1) # 1 means that feature is present\n",
    "                    else:\n",
    "                        features.append(0) # 0 means that feature is not present\n",
    "\n",
    "\n",
    "                feature_values = [] # all features will be on that list   \n",
    "\n",
    "\n",
    "                # first add linkURL\n",
    "                feature_values.append(linkURL)\n",
    "\n",
    "\n",
    "                # add avg. price to feature_values\n",
    "                price_block = link_soup.find_all('ul',attrs={\"class\": \"list-inline features__price-block\"})[0].find_all('li',attrs={\"class\": \"features__price\"})[0]\n",
    "                price = []\n",
    "\n",
    "                if not(str(str(price_block).find(\"features__price--double\")).isdigit()) and not(str(str(price_block).find(\"features__price-old\")).isdigit()):\n",
    "                    for s in price_block.text.split():\n",
    "                        if s.isdigit() or s.replace('.','').isdigit(): # check whether it is int or float\n",
    "                            s = s.replace('.','') # web site uses dot instead of comma. So first destroy dots\n",
    "                            s = s.replace(',','.') # then replace comma with dot because in python decimal numbers indicates w/ dot\n",
    "                            price.append(float(s))\n",
    "                elif str(str(price_block).find(\"features__price--double\")).isdigit():\n",
    "                    # for the price feature, sometimes a range is given. In that case, we'll take average of min and max value of price\n",
    "                    for s in price_block.text.split():\n",
    "                        if s.isdigit() or s.replace('.','').isdigit(): # check whether it is int or float\n",
    "                            s = s.replace('.','')\n",
    "                            s = s.replace(',','.')\n",
    "                            price.append(float(s))\n",
    "                elif str(str(price_block).find(\"features__price-old\")).isdigit():\n",
    "                    start_idx = str(price_block).find('<li class=\"features__price\"><span>') + len('<li class=\"features__price\"><span>')\n",
    "                    end_idx = str(price_block).find(\"</span>\") \n",
    "                    for s in str(price_block)[start_idx:end_idx].split():\n",
    "                        if s.isdigit() or s.replace('.','').isdigit(): # check whether it is int or float\n",
    "                            s = s.replace('.','')\n",
    "                            s = s.replace(',','.')\n",
    "                            price.append(float(s))\n",
    "\n",
    "                feature_values.append(np.mean(price))\n",
    "\n",
    "\n",
    "                # fill the features; locali, superficie, bagni and piano (price is already added.)\n",
    "                loc_sficie_bag = list(map(lambda x: x.text, ul[0].find_all('span',attrs={\"class\": \"text-bold\"})))\n",
    "\n",
    "                j = 0\n",
    "                for i in range(3): # we'll fill locali, superficie and bagni\n",
    "                    if features[i] == 0: # we are checking absence of the feature\n",
    "                        feature_values.append(None) # if it is absent, put it None \n",
    "                    else:\n",
    "                        if i == 0:\n",
    "                            # this part is only for locali. If there is range for locali, take it average ot it\n",
    "                            loc = []\n",
    "                            for e in loc_sficie_bag[j]:\n",
    "                                for s in e.split():\n",
    "                                    if s.isdigit() or s.replace('.','',1).isdigit(): # check whether it is int or float\n",
    "                                        loc.append(float(s))\n",
    "                            feature_values.append(np.mean(loc)) # take it average and add the value to feature_values\n",
    "                            j += 1\n",
    "                        else:\n",
    "                            feature_values.append(int(re.search(r'\\d+', loc_sficie_bag[j]).group())); j += 1\n",
    "\n",
    "\n",
    "                # adding piano; it can be integer or string\n",
    "                piano = ul[0].find_all('abbr',attrs={\"class\": \"text-bold im-abbr\"})\n",
    "\n",
    "                if piano != []: # check whether piano feature does not exist in the link or not\n",
    "                    feature_values.append(piano[0].text.split(\"\\xa0\")[0]) # if it exists, add the value to feature_values \n",
    "                else:\n",
    "                    feature_values.append(None) # if it does not exists, add None to feature_values\n",
    "\n",
    "\n",
    "                # adding description\n",
    "                desc = link_soup.find_all('div',attrs={\"id\": \"description\"})[0].find_all('div',attrs={\"class\": \"col-xs-12 description-text text-compressed\"})[0].text\n",
    "                feature_values.append(desc)\n",
    "\n",
    "                data.loc[data.shape[0]+1]= feature_values # add all features as new row\n",
    "\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            pickle.dump(data, fileObject) # save the dataframe that we got for just 1 page\n",
    "            time.sleep(0.5) # this helps to prevent the website block\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    fileObject.close()\n",
    "    \n",
    "    # read the data part by part\n",
    "    ADM_HW4_data = pd.DataFrame(columns=[\"url\",\"price\", \"locali\", \"superficie\", \"bagni\", \"piano\", \"description\"]) # rename columns\n",
    "\n",
    "    fileObject = open(path + \"adm-hw4-data-dene\",'rb') # open the file to read\n",
    "\n",
    "    # to not force memory, we send the data for each page to pickle. Now, we are collecting them. Since we use try and except,\n",
    "    # some of pages are lost (we have 1729 at the beginning) but around 41000 rows are quite enough also.\n",
    "    for i in range(1,1678+1):\n",
    "        ADM_HW4_data = ADM_HW4_data.append(pickle.load(fileObject))\n",
    "\n",
    "    fileObject.close() # close to file\n",
    "\n",
    "    ADM_HW4_data.reset_index(drop=True, inplace=True) # drop indexes\n",
    "\n",
    "    # since we create data from too many pickle files, I will save it as one piece\n",
    "    ADM_HW4_data.to_pickle(path + 'hw4_data')\n",
    "\n",
    "    # read the data\n",
    "    hw4_data = pd.read_pickle(path + 'hw4_data')\n",
    "    \n",
    "    return hw4_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
